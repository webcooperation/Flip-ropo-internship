{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387582a3",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ab68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Importing selenium webdriver \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#Importing requests\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7baee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1.\n",
      "Title: \"Baby Shark Dance\"[6]\n",
      "Uploader: Pinkfong Baby Shark - Kids' Songs & Stories\n",
      "Views: 13.01\n",
      "Publication date June 17, 2016\n",
      "----------------------------------------\n",
      "Rank: 2.\n",
      "Title: \"Despacito\"[9]\n",
      "Uploader: Luis Fonsi\n",
      "Views: 8.20\n",
      "Publication date January 12, 2017\n",
      "----------------------------------------\n",
      "Rank: 3.\n",
      "Title: \"Johny Johny Yes Papa\"[16]\n",
      "Uploader: LooLoo Kids\n",
      "Views: 6.73\n",
      "Publication date October 8, 2016\n",
      "----------------------------------------\n",
      "Rank: 4.\n",
      "Title: \"Bath Song\"[17]\n",
      "Uploader: Cocomelon – Nursery Rhymes\n",
      "Views: 6.26\n",
      "Publication date May 2, 2018\n",
      "----------------------------------------\n",
      "Rank: 5.\n",
      "Title: \"Shape of You\"[18]\n",
      "Uploader: Ed Sheeran\n",
      "Views: 6.02\n",
      "Publication date January 30, 2017\n",
      "----------------------------------------\n",
      "Rank: 6.\n",
      "Title: \"See You Again\"[21]\n",
      "Uploader: Wiz Khalifa\n",
      "Views: 5.94\n",
      "Publication date April 6, 2015\n",
      "----------------------------------------\n",
      "Rank: 7.\n",
      "Title: \"Phonics Song with Two Words\"[26]\n",
      "Uploader: ChuChu TV\n",
      "Views: 5.36\n",
      "Publication date March 6, 2014\n",
      "----------------------------------------\n",
      "Rank: 8.\n",
      "Title: \"Wheels on the Bus\"[27]\n",
      "Uploader: Cocomelon – Nursery Rhymes\n",
      "Views: 5.36\n",
      "Publication date May 24, 2018\n",
      "----------------------------------------\n",
      "Rank: 9.\n",
      "Title: \"Uptown Funk\"[28]\n",
      "Uploader: Mark Ronson\n",
      "Views: 4.96\n",
      "Publication date November 19, 2014\n",
      "----------------------------------------\n",
      "Rank: 10.\n",
      "Title: \"Learning Colors – Colorful Eggs on a Farm\"[29]\n",
      "Uploader: Miroshka TV\n",
      "Views: 4.91\n",
      "Publication date February 27, 2018\n",
      "----------------------------------------\n",
      "Rank: 11.\n",
      "Title: \"Gangnam Style\"[30]\n",
      "Uploader: Psy\n",
      "Views: 4.83\n",
      "Publication date July 15, 2012\n",
      "----------------------------------------\n",
      "Rank: 12.\n",
      "Title: \"Masha and the Bear – Recipe for Disaster\"[35]\n",
      "Uploader: Get Movies\n",
      "Views: 4.55\n",
      "Publication date January 31, 2012\n",
      "----------------------------------------\n",
      "Rank: 13.\n",
      "Title: \"Dame Tu Cosita\"[36]\n",
      "Uploader: El Chombo\n",
      "Views: 4.38\n",
      "Publication date April 5, 2018\n",
      "----------------------------------------\n",
      "Rank: 14.\n",
      "Title: \"Axel F\"[37]\n",
      "Uploader: Crazy Frog\n",
      "Views: 3.95\n",
      "Publication date June 16, 2009\n",
      "----------------------------------------\n",
      "Rank: 15.\n",
      "Title: \"Sugar\"[38]\n",
      "Uploader: Maroon 5\n",
      "Views: 3.89\n",
      "Publication date January 14, 2015\n",
      "----------------------------------------\n",
      "Rank: 16.\n",
      "Title: \"Roar\"[39]\n",
      "Uploader: Katy Perry\n",
      "Views: 3.82\n",
      "Publication date September 5, 2013\n",
      "----------------------------------------\n",
      "Rank: 17.\n",
      "Title: \"Counting Stars\"[40]\n",
      "Uploader: OneRepublic\n",
      "Views: 3.81\n",
      "Publication date May 31, 2013\n",
      "----------------------------------------\n",
      "Rank: 18.\n",
      "Title: \"Baa Baa Black Sheep\"[41]\n",
      "Uploader: Cocomelon – Nursery Rhymes\n",
      "Views: 3.69\n",
      "Publication date June 25, 2018\n",
      "----------------------------------------\n",
      "Rank: 19.\n",
      "Title: \"Sorry\"[42]\n",
      "Uploader: Justin Bieber\n",
      "Views: 3.67\n",
      "Publication date October 22, 2015\n",
      "----------------------------------------\n",
      "Rank: 20.\n",
      "Title: \"Waka Waka (This Time for Africa)\"[43]\n",
      "Uploader: Shakira\n",
      "Views: 3.63\n",
      "Publication date June 4, 2010\n",
      "----------------------------------------\n",
      "Rank: 21.\n",
      "Title: \"Thinking Out Loud\"[44]\n",
      "Uploader: Ed Sheeran\n",
      "Views: 3.61\n",
      "Publication date October 7, 2014\n",
      "----------------------------------------\n",
      "Rank: 22.\n",
      "Title: \"Lakdi Ki Kathi\"[45]\n",
      "Uploader: Jingle Toons\n",
      "Views: 3.56\n",
      "Publication date June 14, 2018\n",
      "----------------------------------------\n",
      "Rank: 23.\n",
      "Title: \"Dark Horse\"[46]\n",
      "Uploader: Katy Perry\n",
      "Views: 3.54\n",
      "Publication date February 20, 2014\n",
      "----------------------------------------\n",
      "Rank: 24.\n",
      "Title: \"Perfect\"[47]\n",
      "Uploader: Ed Sheeran\n",
      "Views: 3.48\n",
      "Publication date November 9, 2017\n",
      "----------------------------------------\n",
      "Rank: 25.\n",
      "Title: \"Faded\"[48]\n",
      "Uploader: Alan Walker\n",
      "Views: 3.47\n",
      "Publication date December 3, 2015\n",
      "----------------------------------------\n",
      "Rank: 26.\n",
      "Title: \"Let Her Go\"[49]\n",
      "Uploader: Passenger\n",
      "Views: 3.46\n",
      "Publication date July 25, 2012\n",
      "----------------------------------------\n",
      "Rank: 27.\n",
      "Title: \"Humpty the train on a fruits ride\"[50]\n",
      "Uploader: Kiddiestv Hindi – Nursery Rhymes & Kids Songs\n",
      "Views: 3.46\n",
      "Publication date January 26, 2018\n",
      "----------------------------------------\n",
      "Rank: 28.\n",
      "Title: \"Girls Like You\"[51]\n",
      "Uploader: Maroon 5\n",
      "Views: 3.43\n",
      "Publication date May 31, 2018\n",
      "----------------------------------------\n",
      "Rank: 29.\n",
      "Title: \"Bailando\"[52]\n",
      "Uploader: Enrique Iglesias\n",
      "Views: 3.41\n",
      "Publication date April 11, 2014\n",
      "----------------------------------------\n",
      "Rank: 30.\n",
      "Title: \"Lean On\"[53]\n",
      "Uploader: Major Lazer\n",
      "Views: 3.40\n",
      "Publication date March 22, 2015\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object with the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table that contains the video details using class\n",
    "table = soup.find(\"table\", class_=\"wikitable\")\n",
    "\n",
    "# Find all the rows in the table, excluding the header row\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "# Iterate over each row and extract the video details of the first 30\n",
    "for row in rows[0:30]:\n",
    "    try: \n",
    "        # Get the columns of the current row\n",
    "        columns = row.find_all(\"td\")\n",
    "\n",
    "        # Extract the desired information from the columns\n",
    "        rank = columns[0].text.strip()\n",
    "        Name = columns[1].text.strip()\n",
    "        Artist = columns[2].text.strip()\n",
    "        views = columns[3].text.strip()\n",
    "        Pdate=columns[4].text.strip()  \n",
    "        \n",
    "        print(\"Rank:\", rank)\n",
    "        print(\"Title:\", Name)\n",
    "        print(\"Uploader:\", Artist)\n",
    "        print(\"Views:\", views)\n",
    "        print(\"Publication date\",Pdate)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "    except StaleElementReferenceException as e:             # Handling StaleElement Exception   \n",
    "        print(\"Stale Exception\")    \n",
    "         # Print the extracted information\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35a057",
   "metadata": {},
   "source": [
    "3 :Scrape the details of State-wise GDP ofIndia fromstatisticstime.com. \n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aca0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images.google.com\n",
    "driver = webdriver.Chrome()\n",
    "urls=\"http://statisticstimes.com/\"\n",
    "driver.get(urls)\n",
    "\n",
    "#//*[@id=\"top\"]/div[2]/div[2]/button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af58148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "economy = driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/button')   # Click on the economy button by XPATH\n",
    "economy.click()\n",
    "\n",
    "ind = driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/div/a[3]')       # Click on the India button by XPATH\n",
    "ind.click()\n",
    "\n",
    "gdp = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')   # Click on the GDP by states\n",
    "gdp.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125c3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank=[]\n",
    "State =[]\n",
    "GDP=[]\n",
    "GSDP_Current=[]\n",
    "GSDP_Previous=[]\n",
    "Share=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "194e2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Rank \n",
    "r=driver.find_elements(By.XPATH,\"//td[@class='data1']\")\n",
    "for i in r:\n",
    "    try :\n",
    "        Rank.append(i.text)\n",
    "    except:\n",
    "        Rank.append(\"--\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3790b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the State \n",
    "St=driver.find_elements(By.XPATH,\"//td[@class='name']\")\n",
    "for i in St:\n",
    "    try:\n",
    "        State.append(i.text)\n",
    "    except:\n",
    "        State.append(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d58a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the GDP \n",
    "gdp=driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[6]\")\n",
    "for i in gdp:\n",
    "    try:\n",
    "        GDP.append(i.text)\n",
    "    except:\n",
    "        GDP.append(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e485289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Share \n",
    "shr=driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[5]\")\n",
    "for i in shr:\n",
    "    try:\n",
    "        Share.append(i.text)\n",
    "    except:\n",
    "        Share.append(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b35b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the GSDP_Current \n",
    "GSDP=driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[4]\")\n",
    "for i in GSDP:\n",
    "    try:\n",
    "        GSDP_Current.append(i.text)\n",
    "    except:\n",
    "         GSDP_Current.append(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc32b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the GSDP_Previous \n",
    "GSDP_P=driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[8]\")\n",
    "for i in GSDP_P:\n",
    "    try:\n",
    "        GSDP_Previous.append(i.text)\n",
    "    except:\n",
    "         GSDP_Previous.append(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8649fa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>Share In GDP</th>\n",
       "      <th>GDP of State</th>\n",
       "      <th>GSDP_Current</th>\n",
       "      <th>GSDP_Previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>2,039,074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,215,307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,123,982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>1,186,379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,091,077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>739,525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "      <td>942,586</td>\n",
       "      <td>677,428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "      <td>862,957</td>\n",
       "      <td>621,301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "      <td>861,031</td>\n",
       "      <td>612,828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "      <td>809,592</td>\n",
       "      <td>522,009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "      <td>781,653</td>\n",
       "      <td>559,412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "      <td>774,870</td>\n",
       "      <td>590,569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "      <td>734,163</td>\n",
       "      <td>531,085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "      <td>530,363</td>\n",
       "      <td>375,651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "      <td>526,376</td>\n",
       "      <td>397,669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "      <td>487,805</td>\n",
       "      <td>376,877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "      <td>315,881</td>\n",
       "      <td>234,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "      <td>304,063</td>\n",
       "      <td>231,182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "      <td>297,204</td>\n",
       "      <td>224,986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "      <td>245,895</td>\n",
       "      <td>193,273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "      <td>155,956</td>\n",
       "      <td>112,755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "      <td>153,845</td>\n",
       "      <td>117,851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "      <td>73,170</td>\n",
       "      <td>57,787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "      <td>49,845</td>\n",
       "      <td>36,963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "      <td>42,114</td>\n",
       "      <td>31,192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "      <td>34,433</td>\n",
       "      <td>23,013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "      <td>33,481</td>\n",
       "      <td>24,682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "      <td>28,723</td>\n",
       "      <td>18,722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "      <td>27,870</td>\n",
       "      <td>19,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "      <td>27,283</td>\n",
       "      <td>17,647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "      <td>24,603</td>\n",
       "      <td>16,676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "      <td>22,287</td>\n",
       "      <td>16,478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State Share In GDP GDP of State GSDP_Current  \\\n",
       "0     1                Maharashtra       13.94%      399.921    2,632,792   \n",
       "1     2                 Tamil Nadu        8.63%      247.629    1,630,208   \n",
       "2     3              Uttar Pradesh        8.39%      240.726    1,584,764   \n",
       "3     4                    Gujarat        7.96%      228.290    1,502,899   \n",
       "4     5                  Karnataka        7.91%      226.806    1,493,127   \n",
       "5     6                West Bengal        5.77%      165.556    1,089,898   \n",
       "6     7                  Rajasthan        4.99%      143.179      942,586   \n",
       "7     8             Andhra Pradesh        4.57%      131.083      862,957   \n",
       "8     9                  Telangana        4.56%      130.791      861,031   \n",
       "9    10             Madhya Pradesh        4.29%      122.977      809,592   \n",
       "10   11                     Kerala        4.14%      118.733      781,653   \n",
       "11   12                      Delhi        4.10%      117.703      774,870   \n",
       "12   13                    Haryana        3.89%      111.519      734,163   \n",
       "13   14                      Bihar        2.81%       80.562      530,363   \n",
       "14   15                     Punjab        2.79%       79.957      526,376   \n",
       "15   16                     Odisha        2.58%       74.098      487,805   \n",
       "16   17                      Assam        1.67%       47.982      315,881   \n",
       "17   18               Chhattisgarh        1.61%       46.187      304,063   \n",
       "18   19                  Jharkhand        1.57%       45.145      297,204   \n",
       "19   20                Uttarakhand        1.30%       37.351      245,895   \n",
       "20   21            Jammu & Kashmir        0.83%       23.690      155,956   \n",
       "21   22           Himachal Pradesh        0.81%       23.369      153,845   \n",
       "22   23                        Goa        0.39%       11.115       73,170   \n",
       "23   24                    Tripura        0.26%        7.571       49,845   \n",
       "24   25                 Chandigarh        0.22%        6.397       42,114   \n",
       "25   26                 Puducherry        0.18%        5.230       34,433   \n",
       "26   27                  Meghalaya        0.18%        5.086       33,481   \n",
       "27   28                     Sikkim        0.15%        4.363       28,723   \n",
       "28   29                    Manipur        0.15%        4.233       27,870   \n",
       "29   30                   Nagaland        0.14%        4.144       27,283   \n",
       "30   31          Arunachal Pradesh        0.13%        3.737       24,603   \n",
       "31   32                    Mizoram        0.12%        3.385       22,287   \n",
       "32   33  Andaman & Nicobar Islands            -            -            -   \n",
       "\n",
       "   GSDP_Previous  \n",
       "0      2,039,074  \n",
       "1      1,215,307  \n",
       "2      1,123,982  \n",
       "3      1,186,379  \n",
       "4      1,091,077  \n",
       "5        739,525  \n",
       "6        677,428  \n",
       "7        621,301  \n",
       "8        612,828  \n",
       "9        522,009  \n",
       "10       559,412  \n",
       "11       590,569  \n",
       "12       531,085  \n",
       "13       375,651  \n",
       "14       397,669  \n",
       "15       376,877  \n",
       "16       234,048  \n",
       "17       231,182  \n",
       "18       224,986  \n",
       "19       193,273  \n",
       "20       112,755  \n",
       "21       117,851  \n",
       "22        57,787  \n",
       "23        36,963  \n",
       "24        31,192  \n",
       "25        23,013  \n",
       "26        24,682  \n",
       "27        18,722  \n",
       "28        19,300  \n",
       "29        17,647  \n",
       "30        16,676  \n",
       "31        16,478  \n",
       "32             -  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "State_GDP=pd.DataFrame([])\n",
    "State_GDP['Rank']=Rank[:33]\n",
    "State_GDP['State']=State[:33]\n",
    "State_GDP['Share In GDP']=Share[:33]\n",
    "State_GDP['GDP of State']=GDP[:33]\n",
    "State_GDP['GSDP_Current']=GSDP_Current[:33]\n",
    "State_GDP['GSDP_Previous']=GSDP_Previous[:33]\n",
    "State_GDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9f9b7",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa92578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images.google.com\n",
    "driver = webdriver.Chrome()\n",
    "urls=\"https://github.com/trending\"\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e35690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Repository_title =[]\n",
    "Repository_descriptiom=[]\n",
    "Contributor_count=[]\n",
    "Language=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c2bf88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "#scraping the Repositor_Name \n",
    "RN=driver.find_elements(By.XPATH,\"//span[@class='text-normal']\")\n",
    "for i in RN:\n",
    "    try:\n",
    "        Repository_title.append(i.text)\n",
    "    except:\n",
    "         Repository_title.append(\"--\")\n",
    "print(len(Repository_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26528129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "#scraping the Description \n",
    "des=driver.find_elements(By.XPATH,\"//p[@class='col-9 color-fg-muted my-1 pr-4']\")\n",
    "for i in des:\n",
    "    try:\n",
    "        Repository_descriptiom.append(i.text)\n",
    "    except:\n",
    "         Repository_descriptiom.append(\"--\")\n",
    "print(len(Repository_descriptiom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07f3a7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "#scraping the Language \n",
    "Lang=driver.find_elements(By.XPATH,\"//span[@itemprop='programmingLanguage']\")\n",
    "for i in Lang:\n",
    "    try:\n",
    "        Language.append(i.text)\n",
    "    except:\n",
    "        Language.append(\"--\")\n",
    "print(len(Language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "474367ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "#scraping the Muted_Link And Star \n",
    "ml=driver.find_elements(By.XPATH,\"//a[@class='Link--muted d-inline-block mr-3']\")\n",
    "for i in ml:\n",
    "    try:\n",
    "        Contributor_count.append(i.text)\n",
    "    except:\n",
    "         Contributor_count.append(\"--\")\n",
    "print(len(Contributor_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f68c3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Language</th>\n",
       "      <th>Conutrybuted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ixahmedxi /</td>\n",
       "      <td>Open Source Education Platform</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>3,393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xpayne /</td>\n",
       "      <td>Easily migrate your codebase from one framewor...</td>\n",
       "      <td>Python</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public-apis /</td>\n",
       "      <td>A collective list of free APIs</td>\n",
       "      <td>Python</td>\n",
       "      <td>5,222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>geekan /</td>\n",
       "      <td>The Multi-Agent Meta Programming Framework: Gi...</td>\n",
       "      <td>Python</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PKU-YuanGroup /</td>\n",
       "      <td>中文法律大模型</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>247,452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gibbok /</td>\n",
       "      <td>The Concise TypeScript Book: A Concise Guide t...</td>\n",
       "      <td>C++</td>\n",
       "      <td>28,409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>li-plus /</td>\n",
       "      <td>C++ implementation of ChatGLM-6B &amp; ChatGLM2-6B</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>1,482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tohrusky /</td>\n",
       "      <td>2^x Image Super-Resolution</td>\n",
       "      <td>C++</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ztxz16 /</td>\n",
       "      <td>纯c++的全平台llm加速库，支持python调用，chatglm-6B级模型单卡可达100...</td>\n",
       "      <td>Dart</td>\n",
       "      <td>3,334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GopeedLab /</td>\n",
       "      <td>High speed downloader that supports all platfo...</td>\n",
       "      <td>Rust</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>karanpratapsingh /</td>\n",
       "      <td>Learn how to design systems at scale and prepa...</td>\n",
       "      <td>Python</td>\n",
       "      <td>2,428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FuelLabs /</td>\n",
       "      <td>🌴 Empowering everyone to build reliable and ef...</td>\n",
       "      <td>Rust</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pittcsc /</td>\n",
       "      <td>Collection of Summer 2023 &amp; Summer 2024 tech i...</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>donnemartin /</td>\n",
       "      <td>Learn how to design large-scale systems. Prep ...</td>\n",
       "      <td>Python</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FuelLabs /</td>\n",
       "      <td>Rust full node implementation of the Fuel v2 p...</td>\n",
       "      <td>Python</td>\n",
       "      <td>1,794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bacen /</td>\n",
       "      <td>Documentação e arquivos de configuração para p...</td>\n",
       "      <td>Python</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BradyFU /</td>\n",
       "      <td>✨✨Latest Papers and Datasets on Multimodal Lar...</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MunGell /</td>\n",
       "      <td>A list of awesome beginners-friendly projects.</td>\n",
       "      <td>Lua</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>python /</td>\n",
       "      <td>Optional static typing for Python</td>\n",
       "      <td>Python</td>\n",
       "      <td>5,409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name                                        Description  \\\n",
       "0          ixahmedxi /                     Open Source Education Platform   \n",
       "1            0xpayne /  Easily migrate your codebase from one framewor...   \n",
       "2        public-apis /                     A collective list of free APIs   \n",
       "3             geekan /  The Multi-Agent Meta Programming Framework: Gi...   \n",
       "4      PKU-YuanGroup /                                            中文法律大模型   \n",
       "5             gibbok /  The Concise TypeScript Book: A Concise Guide t...   \n",
       "6            li-plus /     C++ implementation of ChatGLM-6B & ChatGLM2-6B   \n",
       "7           Tohrusky /                         2^x Image Super-Resolution   \n",
       "8             ztxz16 /  纯c++的全平台llm加速库，支持python调用，chatglm-6B级模型单卡可达100...   \n",
       "9          GopeedLab /  High speed downloader that supports all platfo...   \n",
       "10  karanpratapsingh /  Learn how to design systems at scale and prepa...   \n",
       "11          FuelLabs /  🌴 Empowering everyone to build reliable and ef...   \n",
       "12           pittcsc /  Collection of Summer 2023 & Summer 2024 tech i...   \n",
       "13       donnemartin /  Learn how to design large-scale systems. Prep ...   \n",
       "14          FuelLabs /  Rust full node implementation of the Fuel v2 p...   \n",
       "15             bacen /  Documentação e arquivos de configuração para p...   \n",
       "16           BradyFU /  ✨✨Latest Papers and Datasets on Multimodal Lar...   \n",
       "17           MunGell /     A list of awesome beginners-friendly projects.   \n",
       "18            python /                  Optional static typing for Python   \n",
       "\n",
       "      Language Conutrybuted  \n",
       "0   TypeScript        3,393  \n",
       "1       Python          239  \n",
       "2       Python        5,222  \n",
       "3       Python          446  \n",
       "4   TypeScript      247,452  \n",
       "5          C++       28,409  \n",
       "6   TypeScript        1,482  \n",
       "7          C++          204  \n",
       "8         Dart        3,334  \n",
       "9         Rust          288  \n",
       "10      Python        2,428  \n",
       "11        Rust          193  \n",
       "12  TypeScript          521  \n",
       "13      Python          143  \n",
       "14      Python        1,794  \n",
       "15      Python          217  \n",
       "16  TypeScript          722  \n",
       "17         Lua           92  \n",
       "18      Python        5,409  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trending_Repository=pd.DataFrame([])\n",
    "Trending_Repository['Name']=Repository_title[:19]\n",
    "Trending_Repository['Description']=Repository_descriptiom[:19]\n",
    "Trending_Repository['Language']=Language[:19]\n",
    "Trending_Repository['Conutrybuted']=Contributor_count[:19]\n",
    "Trending_Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91032b6",
   "metadata": {},
   "source": [
    " 6 :Scrape the details of Highest sellingnovels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781f710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images.google.com\n",
    "driver = webdriver.Chrome()\n",
    "urls=\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed401b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bname=[]\n",
    "Aname=[]\n",
    "Vsold=[]\n",
    "Publisher=[]\n",
    "Genre=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "092a70aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the Name of the book by XPATH\n",
    "Bnames=driver.find_elements(By.XPATH,\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[2]\")\n",
    "for i in Bnames:\n",
    "    if i.text is None :\n",
    "        Bname.append(\"--\") \n",
    "    else:\n",
    "        Bname.append(i.text)\n",
    "print(len(Bname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb0baab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the Name of the book Author by Xpath\n",
    "Anames=driver.find_elements(By.XPATH,\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[3]\")\n",
    "for i in Anames:\n",
    "    if i.text is None :\n",
    "        Aname.append(\"--\") \n",
    "    else:\n",
    "        Aname.append(i.text)\n",
    "print(len(Aname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be51c8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping  the volume of the book sold by Xpath\n",
    "Vsolds=driver.find_elements(By.XPATH,\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[4]\")\n",
    "for i in Vsolds:\n",
    "    if i.text is None :\n",
    "        Vsold.append(\"--\") \n",
    "    else:\n",
    "        Vsold.append(i.text)\n",
    "print(len(Vsold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d4c4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the Name of  publisher  by Xpath\n",
    "PS=driver.find_elements(By.XPATH,\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[5]\")\n",
    "for i in PS:\n",
    "    if i.text is None :\n",
    "        Publisher.append(\"--\") \n",
    "    else:\n",
    "        Publisher.append(i.text)\n",
    "print(len(Publisher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93403cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the novel genre  by Xpath\n",
    "genres=driver.find_elements(By.XPATH,\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/tbody/tr/td[5]\")\n",
    "for i in genres:\n",
    "    if i.text is None :\n",
    "        Genre.append(\"--\") \n",
    "    else:\n",
    "        Genre.append(i.text)\n",
    "print(len(Genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e597de08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book_name</th>\n",
       "      <th>Author_name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Volumes_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>5,094,805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>4,475,152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>4,200,654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>4,179,479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Random House</td>\n",
       "      <td>3,758,936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harry Potter and the Goblet of Fire</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>3,583,215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Harry Potter and the Chamber of Secrets</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>3,484,047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>3,377,906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Angels and Demons</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>3,193,946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Harry Potter and the Half-blood Prince:Childre...</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>2,950,264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fifty Shades Darker</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Random House</td>\n",
       "      <td>2,479,784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>Meyer, Stephenie</td>\n",
       "      <td>Little, Brown Book</td>\n",
       "      <td>Little, Brown Book</td>\n",
       "      <td>2,315,405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Girl with the Dragon Tattoo,The:Millennium Tri...</td>\n",
       "      <td>Larsson, Stieg</td>\n",
       "      <td>Quercus</td>\n",
       "      <td>Quercus</td>\n",
       "      <td>2,233,570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fifty Shades Freed</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Random House</td>\n",
       "      <td>2,193,928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lost Symbol,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>2,183,031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>New Moon</td>\n",
       "      <td>Meyer, Stephenie</td>\n",
       "      <td>Little, Brown Book</td>\n",
       "      <td>Little, Brown Book</td>\n",
       "      <td>2,152,737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Deception Point</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>2,062,145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Eclipse</td>\n",
       "      <td>Meyer, Stephenie</td>\n",
       "      <td>Little, Brown Book</td>\n",
       "      <td>Little, Brown Book</td>\n",
       "      <td>2,052,876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lovely Bones,The</td>\n",
       "      <td>Sebold, Alice</td>\n",
       "      <td>Pan Macmillan</td>\n",
       "      <td>Pan Macmillan</td>\n",
       "      <td>2,005,598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Curious Incident of the Dog in the Night-time,The</td>\n",
       "      <td>Haddon, Mark</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Random House</td>\n",
       "      <td>1,979,552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Digital Fortress</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>1,928,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Short History of Nearly Everything,A</td>\n",
       "      <td>Bryson, Bill</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>1,852,919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Girl Who Played with Fire,The:Millennium Trilogy</td>\n",
       "      <td>Larsson, Stieg</td>\n",
       "      <td>Quercus</td>\n",
       "      <td>Quercus</td>\n",
       "      <td>1,814,784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Breaking Dawn</td>\n",
       "      <td>Meyer, Stephenie</td>\n",
       "      <td>Little, Brown Book</td>\n",
       "      <td>Little, Brown Book</td>\n",
       "      <td>1,787,118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Very Hungry Caterpillar,The:The Very Hungry Ca...</td>\n",
       "      <td>Carle, Eric</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>1,783,535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book_name       Author_name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "5                 Harry Potter and the Goblet of Fire     Rowling, J.K.   \n",
       "6             Harry Potter and the Chamber of Secrets     Rowling, J.K.   \n",
       "7            Harry Potter and the Prisoner of Azkaban     Rowling, J.K.   \n",
       "8                                   Angels and Demons        Brown, Dan   \n",
       "9   Harry Potter and the Half-blood Prince:Childre...     Rowling, J.K.   \n",
       "10                                Fifty Shades Darker      James, E. L.   \n",
       "11                                           Twilight  Meyer, Stephenie   \n",
       "12  Girl with the Dragon Tattoo,The:Millennium Tri...    Larsson, Stieg   \n",
       "13                                 Fifty Shades Freed      James, E. L.   \n",
       "14                                    Lost Symbol,The        Brown, Dan   \n",
       "15                                           New Moon  Meyer, Stephenie   \n",
       "16                                    Deception Point        Brown, Dan   \n",
       "17                                            Eclipse  Meyer, Stephenie   \n",
       "18                                   Lovely Bones,The     Sebold, Alice   \n",
       "19  Curious Incident of the Dog in the Night-time,The      Haddon, Mark   \n",
       "20                                   Digital Fortress        Brown, Dan   \n",
       "21               Short History of Nearly Everything,A      Bryson, Bill   \n",
       "22   Girl Who Played with Fire,The:Millennium Trilogy    Larsson, Stieg   \n",
       "23                                      Breaking Dawn  Meyer, Stephenie   \n",
       "24  Very Hungry Caterpillar,The:The Very Hungry Ca...       Carle, Eric   \n",
       "\n",
       "                 Genre           Publisher Volumes_sold  \n",
       "0           Transworld          Transworld    5,094,805  \n",
       "1           Bloomsbury          Bloomsbury    4,475,152  \n",
       "2           Bloomsbury          Bloomsbury    4,200,654  \n",
       "3           Bloomsbury          Bloomsbury    4,179,479  \n",
       "4         Random House        Random House    3,758,936  \n",
       "5           Bloomsbury          Bloomsbury    3,583,215  \n",
       "6           Bloomsbury          Bloomsbury    3,484,047  \n",
       "7           Bloomsbury          Bloomsbury    3,377,906  \n",
       "8           Transworld          Transworld    3,193,946  \n",
       "9           Bloomsbury          Bloomsbury    2,950,264  \n",
       "10        Random House        Random House    2,479,784  \n",
       "11  Little, Brown Book  Little, Brown Book    2,315,405  \n",
       "12             Quercus             Quercus    2,233,570  \n",
       "13        Random House        Random House    2,193,928  \n",
       "14          Transworld          Transworld    2,183,031  \n",
       "15  Little, Brown Book  Little, Brown Book    2,152,737  \n",
       "16          Transworld          Transworld    2,062,145  \n",
       "17  Little, Brown Book  Little, Brown Book    2,052,876  \n",
       "18       Pan Macmillan       Pan Macmillan    2,005,598  \n",
       "19        Random House        Random House    1,979,552  \n",
       "20          Transworld          Transworld    1,928,900  \n",
       "21          Transworld          Transworld    1,852,919  \n",
       "22             Quercus             Quercus    1,814,784  \n",
       "23  Little, Brown Book  Little, Brown Book    1,787,118  \n",
       "24             Penguin             Penguin    1,783,535  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bk_Combined=pd.DataFrame([]) # Create a data frame to store the retrieved books\n",
    "bk_Combined['Book_name']=Bname[:25]  # first 25 scrapped data\n",
    "bk_Combined['Author_name']=Aname[:25] # first 25 scrapped data\n",
    "bk_Combined['Genre']=Genre[:25] # first 25 scrapped data\n",
    "bk_Combined['Publisher']=Publisher[:25] # first 25 scrapped data\n",
    "bk_Combined['Volumes_sold']=Vsold[:25] # first 25 scrapped data\n",
    "bk_Combined # display only the 25 scrapped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d3323",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f8b0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images.google.com\n",
    "driver = webdriver.Chrome()\n",
    "urls=\"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "353c4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mname=[]\n",
    "Myear=[]\n",
    "genre=[]\n",
    "M_RT=[]\n",
    "Ratings=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5777bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the Movies Name\n",
    "Mnames=driver.find_elements(By.XPATH,\"//div[@class='lister-item-content']/h3/a\")\n",
    "for i in Mnames:\n",
    "    try:\n",
    "        Mname.append(i.text)\n",
    "    except:\n",
    "        Mname.append(\"--\")\n",
    "print(len(Mname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7a1ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the year the movie is produced\n",
    "Myears=driver.find_elements(By.XPATH,\"//span[@class='lister-item-year text-muted unbold']\")\n",
    "for i in Myears:\n",
    "    try:\n",
    "        Myear.append(i.text)\n",
    "    except:\n",
    "        Myear.append(\"--\")\n",
    "print(len(Myear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "167216d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the genre of the movies\n",
    "genres=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small']/span[5]\")\n",
    "for i in genres:\n",
    "    try:\n",
    "        genre.append(i.text)\n",
    "    except:\n",
    "        genre.append(\"--\")\n",
    "print(len(genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28e55f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the run time  of the movies\n",
    "M_RTS=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small']/span[3]\")\n",
    "for i in M_RTS:\n",
    "    try:\n",
    "        M_RT.append(i.text)\n",
    "    except:\n",
    "        M_RT.append(\"--\")\n",
    "print(len(M_RT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48bcec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the Ratings of the movies\n",
    "Rating=driver.find_elements(By.XPATH,\"//div[@class='ipl-rating-star small']/span[2]\")\n",
    "for i in Rating:\n",
    "    try:\n",
    "        Ratings.append(i.text)\n",
    "    except:\n",
    "        Ratings.append(\"--\")\n",
    "print(len(Ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7eeec53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#scraping the votes  of the movies\n",
    "voting=[]\n",
    "votings=driver.find_elements(By.XPATH,\"//div[@class='lister-item-content']/p[4]/span[2]\")\n",
    "for i in votings:\n",
    "    try:\n",
    "        voting.append(i.text)\n",
    "    except:\n",
    "        voting.append(\"--\")\n",
    "print(len(voting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc451a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_name</th>\n",
       "      <th>Movie_year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run_time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>voting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,176,698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2024)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,253,938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,033,746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>303,922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>263,092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Orange Is the New Black</td>\n",
       "      <td>(2013–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>59 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>310,904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Riverdale</td>\n",
       "      <td>(2017–2023)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>45 min</td>\n",
       "      <td>6.5</td>\n",
       "      <td>149,648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grey's Anatomy</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>41 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>324,284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Flash</td>\n",
       "      <td>(2014–2023)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>360,210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arrow</td>\n",
       "      <td>(2012–2020)</td>\n",
       "      <td>Action, Adventure, Crime</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>438,962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Money Heist</td>\n",
       "      <td>(2017–2021)</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>70 min</td>\n",
       "      <td>8.2</td>\n",
       "      <td>499,696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Big Bang Theory</td>\n",
       "      <td>(2007–2019)</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.2</td>\n",
       "      <td>832,965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Black Mirror</td>\n",
       "      <td>(2011– )</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>60 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>594,101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sherlock</td>\n",
       "      <td>(2010–2017)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>88 min</td>\n",
       "      <td>9.1</td>\n",
       "      <td>955,770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Vikings</td>\n",
       "      <td>(2013–2020)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.5</td>\n",
       "      <td>554,712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pretty Little Liars</td>\n",
       "      <td>(2010–2017)</td>\n",
       "      <td>Drama, Mystery, Romance</td>\n",
       "      <td>44 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>172,693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Vampire Diaries</td>\n",
       "      <td>(2009–2017)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>333,183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>American Horror Story</td>\n",
       "      <td>(2011– )</td>\n",
       "      <td>Drama, Horror, Sci-Fi</td>\n",
       "      <td>60 min</td>\n",
       "      <td>8</td>\n",
       "      <td>328,398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Breaking Bad</td>\n",
       "      <td>(2008–2013)</td>\n",
       "      <td>Crime, Drama, Thriller</td>\n",
       "      <td>49 min</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1,997,084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lucifer</td>\n",
       "      <td>(2016–2021)</td>\n",
       "      <td>Crime, Drama, Fantasy</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>338,456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Supernatural</td>\n",
       "      <td>(2005–2020)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.4</td>\n",
       "      <td>461,480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Prison Break</td>\n",
       "      <td>(2005–2017)</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.3</td>\n",
       "      <td>554,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How to Get Away with Murder</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>43 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>158,535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Teen Wolf</td>\n",
       "      <td>(2011–2017)</td>\n",
       "      <td>Action, Drama, Fantasy</td>\n",
       "      <td>41 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>156,762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Simpsons</td>\n",
       "      <td>(1989– )</td>\n",
       "      <td>Animation, Comedy</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>419,888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Once Upon a Time</td>\n",
       "      <td>(2011–2018)</td>\n",
       "      <td>Adventure, Fantasy, Romance</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>230,313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Narcos</td>\n",
       "      <td>(2015–2017)</td>\n",
       "      <td>Biography, Crime, Drama</td>\n",
       "      <td>49 min</td>\n",
       "      <td>8.8</td>\n",
       "      <td>445,389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Daredevil</td>\n",
       "      <td>(2015–2018)</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>54 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>455,661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Friends</td>\n",
       "      <td>(1994–2004)</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1,032,607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>How I Met Your Mother</td>\n",
       "      <td>(2005–2014)</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.3</td>\n",
       "      <td>703,933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Movie_name   Movie_year                        Genre  \\\n",
       "0               Game of Thrones  (2011–2019)     Action, Adventure, Drama   \n",
       "1               Stranger Things  (2016–2024)       Drama, Fantasy, Horror   \n",
       "2              The Walking Dead  (2010–2022)      Drama, Horror, Thriller   \n",
       "3                13 Reasons Why  (2017–2020)     Drama, Mystery, Thriller   \n",
       "4                       The 100  (2014–2020)       Drama, Mystery, Sci-Fi   \n",
       "5       Orange Is the New Black  (2013–2019)         Comedy, Crime, Drama   \n",
       "6                     Riverdale  (2017–2023)        Crime, Drama, Mystery   \n",
       "7                Grey's Anatomy     (2005– )               Drama, Romance   \n",
       "8                     The Flash  (2014–2023)     Action, Adventure, Drama   \n",
       "9                         Arrow  (2012–2020)     Action, Adventure, Crime   \n",
       "10                  Money Heist  (2017–2021)         Action, Crime, Drama   \n",
       "11          The Big Bang Theory  (2007–2019)              Comedy, Romance   \n",
       "12                 Black Mirror     (2011– )       Drama, Mystery, Sci-Fi   \n",
       "13                     Sherlock  (2010–2017)        Crime, Drama, Mystery   \n",
       "14                      Vikings  (2013–2020)     Action, Adventure, Drama   \n",
       "15          Pretty Little Liars  (2010–2017)      Drama, Mystery, Romance   \n",
       "16          The Vampire Diaries  (2009–2017)       Drama, Fantasy, Horror   \n",
       "17        American Horror Story     (2011– )        Drama, Horror, Sci-Fi   \n",
       "18                 Breaking Bad  (2008–2013)       Crime, Drama, Thriller   \n",
       "19                      Lucifer  (2016–2021)        Crime, Drama, Fantasy   \n",
       "20                 Supernatural  (2005–2020)       Drama, Fantasy, Horror   \n",
       "21                 Prison Break  (2005–2017)         Action, Crime, Drama   \n",
       "22  How to Get Away with Murder  (2014–2020)        Crime, Drama, Mystery   \n",
       "23                    Teen Wolf  (2011–2017)       Action, Drama, Fantasy   \n",
       "24                 The Simpsons     (1989– )            Animation, Comedy   \n",
       "25             Once Upon a Time  (2011–2018)  Adventure, Fantasy, Romance   \n",
       "26                       Narcos  (2015–2017)      Biography, Crime, Drama   \n",
       "27                    Daredevil  (2015–2018)         Action, Crime, Drama   \n",
       "28                      Friends  (1994–2004)              Comedy, Romance   \n",
       "29        How I Met Your Mother  (2005–2014)       Comedy, Drama, Romance   \n",
       "\n",
       "   Run_time Ratings     voting  \n",
       "0    57 min     9.2  2,176,698  \n",
       "1    51 min     8.7  1,253,938  \n",
       "2    44 min     8.1  1,033,746  \n",
       "3    60 min     7.5    303,922  \n",
       "4    43 min     7.6    263,092  \n",
       "5    59 min     8.1    310,904  \n",
       "6    45 min     6.5    149,648  \n",
       "7    41 min     7.6    324,284  \n",
       "8    43 min     7.5    360,210  \n",
       "9    42 min     7.5    438,962  \n",
       "10   70 min     8.2    499,696  \n",
       "11   22 min     8.2    832,965  \n",
       "12   60 min     8.7    594,101  \n",
       "13   88 min     9.1    955,770  \n",
       "14   44 min     8.5    554,712  \n",
       "15   44 min     7.4    172,693  \n",
       "16   43 min     7.7    333,183  \n",
       "17   60 min       8    328,398  \n",
       "18   49 min     9.5  1,997,084  \n",
       "19   42 min     8.1    338,456  \n",
       "20   44 min     8.4    461,480  \n",
       "21   44 min     8.3    554,890  \n",
       "22   43 min     8.1    158,535  \n",
       "23   41 min     7.7    156,762  \n",
       "24   22 min     8.7    419,888  \n",
       "25   60 min     7.7    230,313  \n",
       "26   49 min     8.8    445,389  \n",
       "27   54 min     8.6    455,661  \n",
       "28   22 min     8.9  1,032,607  \n",
       "29   22 min     8.3    703,933  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MVI_Combined=pd.DataFrame([]) # Create a data frame to store the retrieved books\n",
    "MVI_Combined['Movie_name']=Mname[:30]  # first 30 scrapped data\n",
    "MVI_Combined['Movie_year']=Myear[:30] # first 30 scrapped data\n",
    "MVI_Combined['Genre']=genre[:30] # first 30 scrapped data\n",
    "MVI_Combined['Run_time']=M_RT[:30] # first 30 scrapped data\n",
    "MVI_Combined['Ratings']=Ratings[:30] # first 30 scrapped data\n",
    "MVI_Combined['voting']=voting[:30] # first 30 scrapped data\n",
    "MVI_Combined # display only the 25 scrapped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491750d",
   "metadata": {},
   "source": [
    "8. Details of Datasetsfrom UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2cf8e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the web page\n",
    "driver = webdriver.Chrome()\n",
    "urls=\"https://archive.ics.uci.edu/\"\n",
    "driver.get(urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fceba03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sbutton = driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]')  # Click on Show All Dataset Page\n",
    "Sbutton.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "01e0b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "DName=[]\n",
    "other_info=[]\n",
    "Task=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "581d9f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris', 'Heart Disease', 'Adult', 'Dry Bean Dataset', 'Diabetes', 'Rice (Cammeo and Osmancik)', 'Wine', 'Car Evaluation', 'Breast Cancer Wisconsin (Diagnostic)', 'Mushroom'] 10\n"
     ]
    }
   ],
   "source": [
    "#scraping the Dataset_Name by XPATH\n",
    "DNames=driver.find_elements(By.XPATH,\"//a[@class='link-hover link text-xl font-semibold']\")\n",
    "for i in DNames:\n",
    "    try:\n",
    "        DName.append(i.text)\n",
    "    except:\n",
    "        DName.append(\"--\")\n",
    "print(DName,len(DName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4e2aff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Classification\\nMultivariate\\n150 Instances\\n4 Attributes', 'Classification\\nMultivariate\\n303 Instances\\n13 Attributes', 'Classification\\nMultivariate\\n48.84K Instances\\n14 Attributes', 'Classification\\nMultivariate\\n13.61K Instances\\n17 Attributes', '20 Attributes', 'Classification\\nMultivariate\\n3.81K Instances\\n8 Attributes', 'Classification\\nMultivariate\\n178 Instances\\n13 Attributes', 'Classification\\nMultivariate\\n1.73K Instances\\n6 Attributes', 'Classification\\nMultivariate\\n569 Instances\\n30 Attributes', 'Classification\\nMultivariate\\n8.12K Instances\\n22 Attributes', 'Classification', 'Multivariate', '150 Instances', '4 Attributes', 'Classification', 'Multivariate', '303 Instances', '13 Attributes', 'Classification', 'Multivariate', '48.84K Instances', '14 Attributes', 'Classification', 'Multivariate', '13.61K Instances', '17 Attributes', '', '', '', '20 Attributes', 'Classification', 'Multivariate', '3.81K Instances', '8 Attributes', 'Classification', 'Multivariate', '178 Instances', '13 Attributes', 'Classification', 'Multivariate', '1.73K Instances', '6 Attributes', 'Classification', 'Multivariate', '569 Instances', '30 Attributes', 'Classification', 'Multivariate', '8.12K Instances', '22 Attributes', 'Classification', 'Multivariate', '150 Instances', '4 Attributes', 'Classification', 'Multivariate', '303 Instances', '13 Attributes', 'Classification', 'Multivariate', '48.84K Instances', '14 Attributes', 'Classification', 'Multivariate', '13.61K Instances', '17 Attributes', '', '', '', '20 Attributes', 'Classification', 'Multivariate', '3.81K Instances', '8 Attributes', 'Classification', 'Multivariate', '178 Instances', '13 Attributes', 'Classification', 'Multivariate', '1.73K Instances', '6 Attributes', 'Classification', 'Multivariate', '569 Instances', '30 Attributes', 'Classification', 'Multivariate', '8.12K Instances', '22 Attributes', 'Classification', 'Multivariate', '150 Instances', '4 Attributes', 'Classification', 'Multivariate', '303 Instances', '13 Attributes', 'Classification', 'Multivariate', '48.84K Instances', '14 Attributes', 'Classification', 'Multivariate', '13.61K Instances', '17 Attributes', '', '', '', '20 Attributes', 'Classification', 'Multivariate', '3.81K Instances', '8 Attributes', 'Classification', 'Multivariate', '178 Instances', '13 Attributes', 'Classification', 'Multivariate', '1.73K Instances', '6 Attributes', 'Classification', 'Multivariate', '569 Instances', '30 Attributes', 'Classification', 'Multivariate', '8.12K Instances', '22 Attributes', 'Classification\\nMultivariate\\n150 Instances\\n4 Attributes', 'Classification\\nMultivariate\\n303 Instances\\n13 Attributes', 'Classification\\nMultivariate\\n48.84K Instances\\n14 Attributes', 'Classification\\nMultivariate\\n13.61K Instances\\n17 Attributes', '20 Attributes', 'Classification\\nMultivariate\\n3.81K Instances\\n8 Attributes', 'Classification\\nMultivariate\\n178 Instances\\n13 Attributes', 'Classification\\nMultivariate\\n1.73K Instances\\n6 Attributes', 'Classification\\nMultivariate\\n569 Instances\\n30 Attributes', 'Classification\\nMultivariate\\n8.12K Instances\\n22 Attributes'] 10\n"
     ]
    }
   ],
   "source": [
    "#scraping the Dataset_Data type by XPATH\n",
    "#/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/div/div[1]/span\n",
    "DTypes=driver.find_elements(By.XPATH,\"//div[@class='my-2 hidden gap-4 md:grid grid-cols-12']\")\n",
    "for i in DTypes:\n",
    "    try:\n",
    "        other_info.append(i.text)\n",
    "    except:\n",
    "        other_info.append(\"--\")\n",
    "print(DType,len(DTypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d7268ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A small classic dataset from Fisher, 1936. One of the earliest datasets used for evaluation of classification methodologies.', '4 databases: Cleveland, Hungary, Switzerland, and the VA Long Beach', 'Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset.', 'Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.', \"This diabetes dataset is from AIM '94\", \"A total of 3810 rice grain's images were taken for the two species, processed and feature inferences were made. 7 morphological features were obtained for each grain of rice.\", 'Using chemical analysis determine the origin of wines', 'Derived from simple hierarchical decision model, this database may be useful for testing constructive induction and structure discovery methods.', 'Diagnostic Wisconsin Breast Cancer Database.', 'From Audobon Society Field Guide; mushrooms described in terms of physical characteristics; classification: poisonous or edible'] 10\n"
     ]
    }
   ],
   "source": [
    "#scraping the Dataset_Name by XPATH\n",
    "Tasks=driver.find_elements(By.XPATH,\"//p[@class='truncate']\")\n",
    "for i in Tasks:\n",
    "    try:\n",
    "        Task.append(i.text)\n",
    "    except:\n",
    "        Task.append(\"--\")\n",
    "print(Task,len(Task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f94a5763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset_name</th>\n",
       "      <th>other_Info</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris</td>\n",
       "      <td>Classification\\nMultivariate\\n150 Instances\\n4...</td>\n",
       "      <td>A small classic dataset from Fisher, 1936. One...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heart Disease</td>\n",
       "      <td>Classification\\nMultivariate\\n303 Instances\\n1...</td>\n",
       "      <td>4 databases: Cleveland, Hungary, Switzerland, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Classification\\nMultivariate\\n48.84K Instances...</td>\n",
       "      <td>Predict whether income exceeds $50K/yr based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dry Bean Dataset</td>\n",
       "      <td>Classification\\nMultivariate\\n13.61K Instances...</td>\n",
       "      <td>Images of 13,611 grains of 7 different registe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diabetes</td>\n",
       "      <td>20 Attributes</td>\n",
       "      <td>This diabetes dataset is from AIM '94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rice (Cammeo and Osmancik)</td>\n",
       "      <td>Classification\\nMultivariate\\n3.81K Instances\\...</td>\n",
       "      <td>A total of 3810 rice grain's images were taken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wine</td>\n",
       "      <td>Classification\\nMultivariate\\n178 Instances\\n1...</td>\n",
       "      <td>Using chemical analysis determine the origin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Car Evaluation</td>\n",
       "      <td>Classification\\nMultivariate\\n1.73K Instances\\...</td>\n",
       "      <td>Derived from simple hierarchical decision mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Breast Cancer Wisconsin (Diagnostic)</td>\n",
       "      <td>Classification\\nMultivariate\\n569 Instances\\n3...</td>\n",
       "      <td>Diagnostic Wisconsin Breast Cancer Database.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mushroom</td>\n",
       "      <td>Classification\\nMultivariate\\n8.12K Instances\\...</td>\n",
       "      <td>From Audobon Society Field Guide; mushrooms de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Dataset_name  \\\n",
       "0                                  Iris   \n",
       "1                         Heart Disease   \n",
       "2                                 Adult   \n",
       "3                      Dry Bean Dataset   \n",
       "4                              Diabetes   \n",
       "5            Rice (Cammeo and Osmancik)   \n",
       "6                                  Wine   \n",
       "7                        Car Evaluation   \n",
       "8  Breast Cancer Wisconsin (Diagnostic)   \n",
       "9                              Mushroom   \n",
       "\n",
       "                                          other_Info  \\\n",
       "0  Classification\\nMultivariate\\n150 Instances\\n4...   \n",
       "1  Classification\\nMultivariate\\n303 Instances\\n1...   \n",
       "2  Classification\\nMultivariate\\n48.84K Instances...   \n",
       "3  Classification\\nMultivariate\\n13.61K Instances...   \n",
       "4                                      20 Attributes   \n",
       "5  Classification\\nMultivariate\\n3.81K Instances\\...   \n",
       "6  Classification\\nMultivariate\\n178 Instances\\n1...   \n",
       "7  Classification\\nMultivariate\\n1.73K Instances\\...   \n",
       "8  Classification\\nMultivariate\\n569 Instances\\n3...   \n",
       "9  Classification\\nMultivariate\\n8.12K Instances\\...   \n",
       "\n",
       "                                                Task  \n",
       "0  A small classic dataset from Fisher, 1936. One...  \n",
       "1  4 databases: Cleveland, Hungary, Switzerland, ...  \n",
       "2  Predict whether income exceeds $50K/yr based o...  \n",
       "3  Images of 13,611 grains of 7 different registe...  \n",
       "4              This diabetes dataset is from AIM '94  \n",
       "5  A total of 3810 rice grain's images were taken...  \n",
       "6  Using chemical analysis determine the origin o...  \n",
       "7  Derived from simple hierarchical decision mode...  \n",
       "8       Diagnostic Wisconsin Breast Cancer Database.  \n",
       "9  From Audobon Society Field Guide; mushrooms de...  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DST_Combined=pd.DataFrame([]) # Create a data frame to store the retrieved books\n",
    "DST_Combined['Dataset_name']=DName  # first 30 scrapped data\n",
    "DST_Combined['other_Info']=other_info # first 30 scrapped data\n",
    "DST_Combined['Task']=Task # first 30 scrapped data\n",
    "\n",
    "DST_Combined # display only the 25 scrapped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933bc577",
   "metadata": {},
   "source": [
    "9: Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants\n",
    "You have to find the following details: \n",
    "A) Name\n",
    "B) Designation\n",
    "C)Company \n",
    "D)Skills they hire for \n",
    "E) Locatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "74aaa3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the web page\n",
    "driver = webdriver.Chrome()\n",
    "urls=\"https://www.naukri.com/data-science-jobs?k=data%20science\"\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ef09cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "JTitle =[]\n",
    "C_Name=[]\n",
    "Skill=[]\n",
    "Salary=[]\n",
    "Location=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "84b8eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['Sr. Product Manager (Data Science, AI/ML)', 'Sr Product Manager (Data Science/AI/ML)', 'Sr Analyst - Data Science and Analytics', 'Data Science Engineer', 'L&D Trainer - Python & Data Science/Data Analytics', 'Lead - Data Science - Financial Services', 'Head - Data Science & Analytics - Bank', 'Engineer II- Data Science & Analytics', 'Manager/Senior Manager - Data Science', 'Senior Analyst, Data Science', 'Data Science Manager', 'Entry Data Science Specialist', 'AI/ML Developer - Python Programming and Data Science', 'Sr Programmer - Python / Data Science Developer', 'Data Science Analytics Sr Analyst - Data Science', 'Manager Data Science', 'Data Science Engineer', 'Demand Planning & Data Science Manager', 'Software Engineer, Data Science', 'Unit Manager - Payments - Data Science/Senior Unit Manager']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Job_Title \n",
    "jt=driver.find_elements(By.XPATH,\"//a[@class='title ellipsis']\")\n",
    "for i in jt:\n",
    "    try:\n",
    "        JTitle.append(i.text)\n",
    "    except:\n",
    "        JTitle.append(\"--\")\n",
    "print(len(JTitle),JTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "55aa92c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['Blue Yonder', 'Blue Yonder', 'Transunion', 'Arbeit Associates', 'AVE-Promagne Business Solutions', 'Dimensions HRD Consultants', 'Dimensions HRD Consultants', 'Raytheon Technologies', 'Axtria India', 'DUN BRADSTREET INFORMATION SERVICES INDIA PRIVATE LIMITED', 'Conde Nast India', 'Camp Dresser & Mckee', 'Freestone Infotech Private Limited', 'Ajanta Pharma', 'Accenture', 'Mondelez', 'Bizongo', 'Bajaj Electricals', 'Epiq Systems', 'Bajaj Finserv Ltd.']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Company_Name \n",
    "cn=driver.find_elements(By.XPATH,\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in cn:\n",
    "    try:\n",
    "        C_Name.append(i.text)\n",
    "    except:\n",
    "        C_Name.append(\"--\")\n",
    "print(len(C_Name),C_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1e8361da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['Not disclosed', '50,000 PA', 'Not disclosed', '12-18 Lacs PA', 'Not disclosed', 'Not disclosed', 'Not disclosed', 'Not disclosed', 'Not disclosed', 'Not disclosed', '40-55 Lacs PA', 'Not disclosed', 'Not disclosed', '6-12 Lacs PA', 'Not disclosed', 'Not disclosed', '20-30 Lacs PA', '15-20 Lacs PA', 'Not disclosed', 'Not disclosed']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Salary\n",
    "sal=driver.find_elements(By.XPATH,\"//li[@class='fleft br2 placeHolderLi salary']\")\n",
    "for i in sal:\n",
    "    try:\n",
    "        Salary.append(i.text)\n",
    "    except:\n",
    "        Salary.append(\"--\")\n",
    "print(len(Salary),Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a3b502e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['Bangalore/Bengaluru', 'Remote', 'Pune, Chennai', 'Hybrid - Gurgaon/ Gurugram, Haryana, Bangalore/ Bengaluru, Karnataka', 'Kolkata, Mumbai, Hyderabad/Secunderabad, Pune, Chennai, Ahmedabad, Delhi / NCR, Bangalore/Bengaluru', 'Gurgaon/Gurugram', 'Gurgaon/Gurugram', 'Hybrid - Bangalore/ Bengaluru, Karnataka(Yelahanka)', 'Noida, Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram, Bangalore/Bengaluru', 'Hyderabad/Secunderabad', 'Hybrid - Bangalore/ Bengaluru, Karnataka', 'Bangalore/Bengaluru', 'Mumbai (All Areas)', 'Mumbai, Maharashtra, Mumbai Suburban, Maharashtra', 'Mumbai', 'Mumbai', 'Bangalore/Bengaluru', 'Mumbai (All Areas)', 'Hybrid - Hyderabad/Secunderabad', 'Pune']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Location \n",
    "loc=driver.find_elements(By.XPATH,\"//li[@class='fleft br2 placeHolderLi location']/span\")\n",
    "for i in loc:\n",
    "    if i.text is None :\n",
    "        Location.append(\"--\") \n",
    "    else:\n",
    "        Location.append(i.text)\n",
    "print(len(Location),Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f46cbfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['SR\\nMarketing\\nData Science\\nAnalytical\\nManagement\\nMarket research\\nIntelligence\\nDevelopment', 'Data Science\\nproduct manager\\nMachine\\nScience\\nArtificial Intelligence\\nProduct management\\nMachine learning\\nMl', 'Analytics\\nFinance\\nBusiness intelligence\\nQuality control management\\nIntelligence\\nAuditing\\nData\\nOperations', 'SQL\\nPython\\nData Science\\nData analytics\\nTableau\\nProgram\\nScience\\nData', 'Python\\nMachine\\nScience\\nDevelopment\\nMachine learning\\nData Science\\nTraining\\nStatistics', 'Data Science\\nFinancial services\\nScience\\nData modeling\\nData\\nAnalytics\\nFinance\\nData analytics', 'Data Analytics\\nData Science\\nScience\\nData modeling\\nData\\nIntelligence\\nAnalytics\\nBusiness intelligence', 'Data Science\\nstatistical modeling\\nmachine learning\\nDeep Learning\\nsql\\nAgile\\nScience\\nData', 'Data Science\\nNatural language processing\\nData management\\nShiny\\nData Pipeline\\nManagement\\nIntelligence\\nData quality', 'Analysis\\nAnalytical\\nData Science\\nData analysis\\nMonitoring\\nData\\nMachine\\nPython', 'python\\ndata science\\nDeep Learning\\nsql\\ncommunication skills\\nMachine\\nPredictive\\nPredictive modeling', 'Computer vision\\nIntelligence\\nMachine\\nData collection\\nData analysis\\nData\\nNeural networks\\nBusiness intelligence', 'python\\ndata science\\nTensorflow\\nFramework\\nDeep Learning\\nLanguages\\nDevelopment\\nTraining', 'Data Modeling\\nPython\\nStatistics\\nData Science\\nETL\\nNumpy\\nExtraction\\nData extraction', 'Data Science\\nCloud\\nInsights\\nSR\\nBusiness Insight\\nScience\\nData analysis\\nBusiness Insights', 'Pattern recognition\\nSAP\\nData Science\\nAnalytics\\nDesign patterns\\nData\\nMetadata\\nManagement', 'Vision\\nAnalytics\\nDeep Learning\\nNetworking\\nTensorflow\\nIntelligence\\nProcess\\nDesign', 'demand planning\\nAnalytics\\nData\\nPlanning\\nScience\\nData Science\\nForecasting\\nManagement', 'Data Science\\nSpark\\nScikit-learn\\nKeras\\nSoftware development\\nSoftware engineering\\nTensorflow\\nUI development', 'DBMS\\nPython\\nArchitecture\\nData management\\nDatabase design\\nQuality assurance\\nPerformance tuning\\nPerformance']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Skill \n",
    "sk=driver.find_elements(By.XPATH,\"//ul[@class='tags has-description']\")\n",
    "for i in sk:\n",
    "    if i.text is None :\n",
    "        Skill.append(\"--\") \n",
    "    else:\n",
    "        Skill.append(i.text)\n",
    "print(len(Skill),Skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "346cbdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Name</th>\n",
       "      <th>Skill</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr. Product Manager (Data Science, AI/ML)</td>\n",
       "      <td>Blue Yonder</td>\n",
       "      <td>SR\\nMarketing\\nData Science\\nAnalytical\\nManag...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sr Product Manager (Data Science/AI/ML)</td>\n",
       "      <td>Blue Yonder</td>\n",
       "      <td>Data Science\\nproduct manager\\nMachine\\nScienc...</td>\n",
       "      <td>50,000 PA</td>\n",
       "      <td>Remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sr Analyst - Data Science and Analytics</td>\n",
       "      <td>Transunion</td>\n",
       "      <td>Analytics\\nFinance\\nBusiness intelligence\\nQua...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Pune, Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science Engineer</td>\n",
       "      <td>Arbeit Associates</td>\n",
       "      <td>SQL\\nPython\\nData Science\\nData analytics\\nTab...</td>\n",
       "      <td>12-18 Lacs PA</td>\n",
       "      <td>Hybrid - Gurgaon/ Gurugram, Haryana, Bangalore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L&amp;D Trainer - Python &amp; Data Science/Data Analy...</td>\n",
       "      <td>AVE-Promagne Business Solutions</td>\n",
       "      <td>Python\\nMachine\\nScience\\nDevelopment\\nMachine...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lead - Data Science - Financial Services</td>\n",
       "      <td>Dimensions HRD Consultants</td>\n",
       "      <td>Data Science\\nFinancial services\\nScience\\nDat...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Head - Data Science &amp; Analytics - Bank</td>\n",
       "      <td>Dimensions HRD Consultants</td>\n",
       "      <td>Data Analytics\\nData Science\\nScience\\nData mo...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Engineer II- Data Science &amp; Analytics</td>\n",
       "      <td>Raytheon Technologies</td>\n",
       "      <td>Data Science\\nstatistical modeling\\nmachine le...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Hybrid - Bangalore/ Bengaluru, Karnataka(Yelah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Manager/Senior Manager - Data Science</td>\n",
       "      <td>Axtria India</td>\n",
       "      <td>Data Science\\nNatural language processing\\nDat...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Noida, Hyderabad/Secunderabad, Pune, Gurgaon/G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Analyst, Data Science</td>\n",
       "      <td>DUN BRADSTREET INFORMATION SERVICES INDIA PRIV...</td>\n",
       "      <td>Analysis\\nAnalytical\\nData Science\\nData analy...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>Conde Nast India</td>\n",
       "      <td>python\\ndata science\\nDeep Learning\\nsql\\ncomm...</td>\n",
       "      <td>40-55 Lacs PA</td>\n",
       "      <td>Hybrid - Bangalore/ Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Entry Data Science Specialist</td>\n",
       "      <td>Camp Dresser &amp; Mckee</td>\n",
       "      <td>Computer vision\\nIntelligence\\nMachine\\nData c...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AI/ML Developer - Python Programming and Data ...</td>\n",
       "      <td>Freestone Infotech Private Limited</td>\n",
       "      <td>python\\ndata science\\nTensorflow\\nFramework\\nD...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sr Programmer - Python / Data Science Developer</td>\n",
       "      <td>Ajanta Pharma</td>\n",
       "      <td>Data Modeling\\nPython\\nStatistics\\nData Scienc...</td>\n",
       "      <td>6-12 Lacs PA</td>\n",
       "      <td>Mumbai, Maharashtra, Mumbai Suburban, Maharashtra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Science Analytics Sr Analyst - Data Science</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Data Science\\nCloud\\nInsights\\nSR\\nBusiness In...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Manager Data Science</td>\n",
       "      <td>Mondelez</td>\n",
       "      <td>Pattern recognition\\nSAP\\nData Science\\nAnalyt...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data Science Engineer</td>\n",
       "      <td>Bizongo</td>\n",
       "      <td>Vision\\nAnalytics\\nDeep Learning\\nNetworking\\n...</td>\n",
       "      <td>20-30 Lacs PA</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Demand Planning &amp; Data Science Manager</td>\n",
       "      <td>Bajaj Electricals</td>\n",
       "      <td>demand planning\\nAnalytics\\nData\\nPlanning\\nSc...</td>\n",
       "      <td>15-20 Lacs PA</td>\n",
       "      <td>Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Software Engineer, Data Science</td>\n",
       "      <td>Epiq Systems</td>\n",
       "      <td>Data Science\\nSpark\\nScikit-learn\\nKeras\\nSoft...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Hybrid - Hyderabad/Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Unit Manager - Payments - Data Science/Senior ...</td>\n",
       "      <td>Bajaj Finserv Ltd.</td>\n",
       "      <td>DBMS\\nPython\\nArchitecture\\nData management\\nD...</td>\n",
       "      <td>Not disclosed</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Job_Title  \\\n",
       "0           Sr. Product Manager (Data Science, AI/ML)   \n",
       "1             Sr Product Manager (Data Science/AI/ML)   \n",
       "2             Sr Analyst - Data Science and Analytics   \n",
       "3                               Data Science Engineer   \n",
       "4   L&D Trainer - Python & Data Science/Data Analy...   \n",
       "5            Lead - Data Science - Financial Services   \n",
       "6              Head - Data Science & Analytics - Bank   \n",
       "7               Engineer II- Data Science & Analytics   \n",
       "8               Manager/Senior Manager - Data Science   \n",
       "9                        Senior Analyst, Data Science   \n",
       "10                               Data Science Manager   \n",
       "11                      Entry Data Science Specialist   \n",
       "12  AI/ML Developer - Python Programming and Data ...   \n",
       "13    Sr Programmer - Python / Data Science Developer   \n",
       "14   Data Science Analytics Sr Analyst - Data Science   \n",
       "15                               Manager Data Science   \n",
       "16                              Data Science Engineer   \n",
       "17             Demand Planning & Data Science Manager   \n",
       "18                    Software Engineer, Data Science   \n",
       "19  Unit Manager - Payments - Data Science/Senior ...   \n",
       "\n",
       "                                             Job_Name  \\\n",
       "0                                         Blue Yonder   \n",
       "1                                         Blue Yonder   \n",
       "2                                          Transunion   \n",
       "3                                   Arbeit Associates   \n",
       "4                     AVE-Promagne Business Solutions   \n",
       "5                          Dimensions HRD Consultants   \n",
       "6                          Dimensions HRD Consultants   \n",
       "7                               Raytheon Technologies   \n",
       "8                                        Axtria India   \n",
       "9   DUN BRADSTREET INFORMATION SERVICES INDIA PRIV...   \n",
       "10                                   Conde Nast India   \n",
       "11                               Camp Dresser & Mckee   \n",
       "12                 Freestone Infotech Private Limited   \n",
       "13                                      Ajanta Pharma   \n",
       "14                                          Accenture   \n",
       "15                                           Mondelez   \n",
       "16                                            Bizongo   \n",
       "17                                  Bajaj Electricals   \n",
       "18                                       Epiq Systems   \n",
       "19                                 Bajaj Finserv Ltd.   \n",
       "\n",
       "                                                Skill         Salary  \\\n",
       "0   SR\\nMarketing\\nData Science\\nAnalytical\\nManag...  Not disclosed   \n",
       "1   Data Science\\nproduct manager\\nMachine\\nScienc...      50,000 PA   \n",
       "2   Analytics\\nFinance\\nBusiness intelligence\\nQua...  Not disclosed   \n",
       "3   SQL\\nPython\\nData Science\\nData analytics\\nTab...  12-18 Lacs PA   \n",
       "4   Python\\nMachine\\nScience\\nDevelopment\\nMachine...  Not disclosed   \n",
       "5   Data Science\\nFinancial services\\nScience\\nDat...  Not disclosed   \n",
       "6   Data Analytics\\nData Science\\nScience\\nData mo...  Not disclosed   \n",
       "7   Data Science\\nstatistical modeling\\nmachine le...  Not disclosed   \n",
       "8   Data Science\\nNatural language processing\\nDat...  Not disclosed   \n",
       "9   Analysis\\nAnalytical\\nData Science\\nData analy...  Not disclosed   \n",
       "10  python\\ndata science\\nDeep Learning\\nsql\\ncomm...  40-55 Lacs PA   \n",
       "11  Computer vision\\nIntelligence\\nMachine\\nData c...  Not disclosed   \n",
       "12  python\\ndata science\\nTensorflow\\nFramework\\nD...  Not disclosed   \n",
       "13  Data Modeling\\nPython\\nStatistics\\nData Scienc...   6-12 Lacs PA   \n",
       "14  Data Science\\nCloud\\nInsights\\nSR\\nBusiness In...  Not disclosed   \n",
       "15  Pattern recognition\\nSAP\\nData Science\\nAnalyt...  Not disclosed   \n",
       "16  Vision\\nAnalytics\\nDeep Learning\\nNetworking\\n...  20-30 Lacs PA   \n",
       "17  demand planning\\nAnalytics\\nData\\nPlanning\\nSc...  15-20 Lacs PA   \n",
       "18  Data Science\\nSpark\\nScikit-learn\\nKeras\\nSoft...  Not disclosed   \n",
       "19  DBMS\\nPython\\nArchitecture\\nData management\\nD...  Not disclosed   \n",
       "\n",
       "                                             Location  \n",
       "0                                 Bangalore/Bengaluru  \n",
       "1                                              Remote  \n",
       "2                                       Pune, Chennai  \n",
       "3   Hybrid - Gurgaon/ Gurugram, Haryana, Bangalore...  \n",
       "4   Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...  \n",
       "5                                    Gurgaon/Gurugram  \n",
       "6                                    Gurgaon/Gurugram  \n",
       "7   Hybrid - Bangalore/ Bengaluru, Karnataka(Yelah...  \n",
       "8   Noida, Hyderabad/Secunderabad, Pune, Gurgaon/G...  \n",
       "9                              Hyderabad/Secunderabad  \n",
       "10           Hybrid - Bangalore/ Bengaluru, Karnataka  \n",
       "11                                Bangalore/Bengaluru  \n",
       "12                                 Mumbai (All Areas)  \n",
       "13  Mumbai, Maharashtra, Mumbai Suburban, Maharashtra  \n",
       "14                                             Mumbai  \n",
       "15                                             Mumbai  \n",
       "16                                Bangalore/Bengaluru  \n",
       "17                                 Mumbai (All Areas)  \n",
       "18                    Hybrid - Hyderabad/Secunderabad  \n",
       "19                                               Pune  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Info_Combined=pd.DataFrame([]) # Create a data frame to store the retrieved books\n",
    "Info_Combined['Job_Title']=JTitle  \n",
    "Info_Combined['Job_Name']= C_Name \n",
    "Info_Combined['Skill']=Skill \n",
    "Info_Combined['Salary']=Salary\n",
    "Info_Combined['Location']=Location\n",
    "Info_Combined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3cd612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
